{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use PyCox to import the METABRIC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox import datasets\n",
    "df = datasets.metabric.read_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing, setting Random folds and computing Event Quantiles of Interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43.68333435058594, 86.86666870117188, 146.33333587646484, 283.5426806640625]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dat1  = df[['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8']]\n",
    "times = (df['duration'].values+1)\n",
    "events =  df['event'].values\n",
    "data = dat1.to_numpy()\n",
    "folds = np.array([1]*381 + [2]*381 + [3]*381 + [4]*381 + [5]*380 )\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(folds)\n",
    "np.quantile(times[events==1], [0.25, .5, .75, .99]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a flag that is used to artificially increase the amount of censoring in the \n",
    "#dataset to determine robustness of DSM to increased censoring levels.\n",
    "INCREASE_CENSORING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dsm_utilites' from '/Users/chiragn/Research/ICML2020/DeepSurvivalMachines/dsm_utilites.py'>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import dsm\n",
    "import dsm_utilites\n",
    "importlib.reload(dsm)\n",
    "importlib.reload(dsm_utilites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val len: 228\n",
      "tr  len: 1295\n",
      "Censoring in Fold: 0.5814671814671815\n",
      "Censoring in Fold: 0.5814671814671815\n",
      "Pretraining the Underlying Distributions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede4627a9dd342f0ba37184b228b739c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "202.8469264171793 1.2680954189977467\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39716c9dc58548e5afb3f314a08febe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5295747584993196 (0.7436527098215582, 0.6811342904030737, 0.5970621901813846, 0.611855947999837)\n",
      "3.515131386970598 (0.7386963678694428, 0.6716928435166729, 0.5849140201392863, 0.62534764825785)\n",
      "3.5158226769628036 (0.7318455746593371, 0.6699128823791801, 0.5911467483533817, 0.6331405037220301)\n",
      "3.515479804334142 (0.7297725821056623, 0.672073403791732, 0.5936461114676398, 0.6369502882527434)\n",
      "3.5152090747964597 (0.7299993252087804, 0.6723811713623161, 0.5948084129361593, 0.6397806508689314)\n",
      "3.515229667328146 (0.727796010086639, 0.6730755262613353, 0.5972531310617756, 0.6420898707648605)\n",
      "3.515459661315446 (0.7282370222892353, 0.6703104281959925, 0.5979750789683245, 0.6429366444808753)\n",
      "3.5152011552733837 (0.7284817501550502, 0.6689068376100378, 0.5977576124523662, 0.6441363144872003)\n",
      "3.5156224070684705 (0.7298338654512917, 0.6682243802957398, 0.5976506274847705, 0.645576700159252)\n",
      "3.516007568690427 (0.7302225119493331, 0.6672430764040999, 0.5983333250469652, 0.6398588975503352)\n",
      "3.516402141571075 (0.7294949009807973, 0.6672938918473681, 0.5990394217700785, 0.6353057345001489)\n"
     ]
    }
   ],
   "source": [
    "#set parameter grid\n",
    "params = [{'G':6, 'mlptyp':2,'HIDDEN':[100], 'n_iter':int(1000), 'lr':1e-3, 'ELBO':True, 'mean':False, \\\n",
    "           'lambd':0, 'alpha':1,'thres':1e-3, 'bs':int(25)}]\n",
    "\n",
    "\n",
    "#set val data size\n",
    "vsize = int(0.15*1523)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for param in params:\n",
    "\n",
    "    outs = []\n",
    "\n",
    "    for f in range(1,6,1):\n",
    "\n",
    "        x_train = data[folds!=f]\n",
    "        x_test  = data[folds==f]\n",
    "        x_valid = x_train[-vsize:, :]\n",
    "        x_train = x_train[:-vsize, :]\n",
    "\n",
    "        t_train = times[folds!=f]\n",
    "        t_test  = times[folds==f]\n",
    "        t_valid = t_train[-vsize:]\n",
    "        t_train = t_train[:-vsize]\n",
    "\n",
    "\n",
    "        e_train = events[folds!=f]\n",
    "        e_test  = events[folds==f]\n",
    "        e_valid = e_train[-vsize:]\n",
    "        e_train = e_train[:-vsize]\n",
    "\n",
    "\n",
    "        print (\"val len:\", len(x_valid))\n",
    "\n",
    "        print (\"tr  len:\", len(x_train))\n",
    "\n",
    "\n",
    "        #normalize the feature set using standard scaling\n",
    "\n",
    "        scl = StandardScaler()\n",
    "        x_train = scl.fit_transform(x_train)\n",
    "        x_valid = scl.transform(x_valid)\n",
    "        x_test = scl.transform(x_test)\n",
    "\n",
    "\n",
    "        print (\"Censoring in Fold:\", np.mean(e_train))\n",
    "\n",
    "        if INCREASE_CENSORING:\n",
    "            e_train, t_train = increaseCensoring(e_train, t_train, .50)\n",
    "\n",
    "        print (\"Censoring in Fold:\", np.mean(e_train))\n",
    "\n",
    "        #Convert the train, test and validation data torch\n",
    "\n",
    "        x_train = torch.from_numpy(x_train).double() \n",
    "        e_train = torch.from_numpy(e_train).double() \n",
    "        t_train = torch.from_numpy(t_train).double() \n",
    "\n",
    "        x_valid = torch.from_numpy(x_valid).double() \n",
    "        e_valid = torch.from_numpy(e_valid).double() \n",
    "        t_valid = torch.from_numpy(t_valid).double() \n",
    "\n",
    "        x_test = torch.from_numpy(x_test).double() \n",
    "        e_test = torch.from_numpy(e_test).double() \n",
    "        t_test = torch.from_numpy(t_test).double() \n",
    "\n",
    "\n",
    "        K, mlptyp, HIDDEN, n_iter, lr, ELBO, mean, lambd, alpha, thres, bs = \\\n",
    "        param['G'], param['mlptyp'], param['HIDDEN'], param['n_iter'], param['lr'], \\\n",
    "        param['ELBO'], param['mean'], param['lambd'], param['alpha'], param['thres'], param['bs']\n",
    "\n",
    "        D = x_train.shape[1]\n",
    "        \n",
    "        model = dsm.DeepSurvivalMachines(D, K, mlptyp, HIDDEN, dist='Weibull')\n",
    "        model.double()\n",
    "        \n",
    "        model, i = dsm_utilites.trainDSM(model,quantiles,x_train, t_train, e_train, x_valid, t_valid, e_valid,lr=lr,bs=bs,alpha=alpha )\n",
    "        \n",
    "    \n",
    "        print (\"TEST PERFORMANCE\")\n",
    "\n",
    "        out =  (dsm_utilites.computeCIScores(model, quantiles, x_test, t_test, e_test, t_train, e_train))\n",
    "\n",
    "        print (out)\n",
    "\n",
    "        outs.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43.68333435058594, 86.86666870117188, 146.33333587646484, 283.5426806640625]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
